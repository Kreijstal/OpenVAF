/*
 * ******************************************************************************************
 * Copyright (c) 2019 Pascal Kuthe. This file is part of the VARF project.
 * It is subject to the license terms in the LICENSE file found in the top-level directory
 *  of this distribution and at  https://gitlab.com/jamescoding/VARF/blob/master/LICENSE.
 *  No part of VARF, including this file, may be copied, modified, propagated, or
 *  distributed except according to the terms contained in the LICENSE file.
 * *****************************************************************************************
 */

use logos::{Extras, Logos, Slice, Source};

use crate::span::{Index, LineNumber, Range};

#[derive(Clone)]
pub struct LineCount(LineNumber);
impl Extras for LineCount {}
impl Default for LineCount {
    fn default() -> Self {
        Self(0)
    }
}

//in terms of api this just serves as a lexer token enum. however it actually is the real lexer generated by logos. The struct just does some extra handling that logos doesnt allow (easily)
#[derive(Clone, Logos, Debug, PartialEq, Copy)]
#[extras = "LineCount"]
pub enum Token {
    //Newline handling
    #[regex = r"\\\n"]
    MacroDefNewLine,
    #[regex = r"\n"]
    Newline,

    //Actual Tokens

    //required rules
    #[end]
    EOF,
    #[token = "//"]
    #[token = "/*"]
    #[callback = "ignore_comments"]
    #[error]
    Unexpected,
    UnexpectedEOF,
    CommentEnd,
    PreprocessorError,

    //Compiler directives
    #[token = "`include"]
    Include,
    #[token = "`ifdef"]
    MacroIf,
    #[token = "`ifndef"]
    MacroIfn,
    #[token = "`elsif"]
    MacroElsif,
    #[token = "`else"]
    MacroElse,
    #[token = "`endif"]
    MacroEndIf,
    #[token = "`define"]
    MacroDef,
    #[regex = r"`[[:alpha:]_][[:word:]\$]*"]
    MacroReference,

    //Identifiers
    #[regex = r"[[:alpha:]_][[:word:]\$]*"]
    SimpleIdentifier,
    #[regex = r"\\[[:print:]&&\S]+\s"]
    EscapedIdentifier,

    //Constants
    #[regex = r#""([^\n"\\]|\\[\\tn"])*""#]
    LiteralString,
    #[regex = r"[0-9][0-9_]*"]
    LiteralUnsignedNumber,
    #[regex = r"[0-9][0-9_]*\.[0-9][0-9_]*[TGMKkmupfa]"]
    #[regex = r"[0-9][0-9_]*\.[0-9][0-9_]*[eE][+-]?[0-9][0-9_]*"]
    #[regex = r"[0-9][0-9_]*[TGMKkmupfa]"]
    #[regex = r"[0-9][0-9_]*[eE][+-]?[0-9][0-9_]*"]
    #[regex = r"[0-9][0-9_]*\.[0-9][0-9_]*"]
    LiteralRealNumber,

    //Symbols
    #[token = "."]
    Accessor,
    #[token = ";"]
    Semicolon,
    #[token = ":"]
    Colon,
    #[token = ","]
    Comma,
    #[token = "("]
    ParenOpen,
    #[token = ")"]
    ParenClose,
    #[token = "<+"]
    Contribute,
    #[token = "="]
    Assign,

    //Arithmatic Operators
    #[token = "*"]
    OpMul,
    #[token = "/"]
    OpDiv,
    #[token = "%"]
    OpRemain,
    #[token = "+"]
    Plus,
    #[token = "-"]
    Minus,
    #[token = "**"]
    OpExp,
    //UnaryOperators
    #[token = "!"]
    OpLogicNot,
    #[token = "~"]
    OpBitNot,

    #[token = "<<"]
    OpArithmeticShiftLeft,
    #[token = ">>"]
    OpArithmeticShiftRight,

    //Relational
    #[token = "<"]
    OpLT,
    #[token = "<="]
    OpLessEqual,
    #[token = ">"]
    OpGreater,
    #[token = ">="]
    OpGreaterEqual,
    #[token = "=="]
    OpeEqual,
    #[token = "!="]
    OpNotEqual,
    //Logic
    #[token = "&&"]
    OpLogicAnd,
    #[token = "||"]
    OpLogicalOr,

    //Bit
    #[token = "&"]
    OpBitAnd,
    #[token = "^"]
    OpBitXor,
    #[token = "|"]
    OpBitOr,

    //Other
    #[token = "?"]
    OpCondition,
    #[token = "?"]
    OpElse,

    //Keywords
    #[token = "if"]
    If,
    #[token = "else"]
    Else,

    #[token = "begin"]
    Begin,
    #[token = "End"]
    End,
    #[token = "module"]
    Module,
    #[token = "endmodule"]
    EndModule,
    #[token = "branch"]
    Branch,
    #[token = "parameter"]
    Parameter,
    #[token = "localparam"]
    DefineParameter,
    #[token = "defparam"]
    LocalParameter,
    #[token = "analog"]
    Analog,
    #[token = "initial"]
    AnalogInitial,
    #[token = "input"]
    Input,
    #[token = "inout"]
    Inout,
    #[token = "output"]
    Output,

    #[token = "signed"]
    Signed,
    #[token = "vectored"]
    Vectored,
    #[token = "scalared"]
    Scalared,

    #[token = "string"]
    String,
    #[token = "time"]
    Time,
    #[token = "realtime"]
    Realtime,
    #[token = "integer"]
    Integer,
    #[token = "real"]
    Real,

    #[token = "reg"]
    Reg,
    #[token = "wreal"]
    Wreal,

    #[token = "potential"]
    Potential,
    #[token = "flow"]
    Flow,

    #[token = "ddt"]
    TimeDerivative,
    #[token = "ddx"]
    PartialDerivative,
    #[token = "idt"]
    TimeIntegral,
    #[token = "idtmod"]
    TimeIntegralMod,
    #[token = "exp"]
    Exp,
    #[token = "sqrt"]
    Sqrt,
    #[token = "pow"]
    Pow,
    #[token = "white_noise"]
    WhiteNoise,
    #[token = "flicker_noise"]
    FlickerNoise,
    #[token = "abs"]
    Abs,
    #[token = "from"]
    From,
    #[token = "exclude"]
    Exclude,
    #[token = "inf"]
    Infinity,

    #[token = "nature"]
    Nature,
    #[token = "endnature"]
    EndNature,
    #[token = "abstol"]
    Abstol,
    #[token = "access"]
    Access,
    #[token = "ddt_nature"]
    TimeDerivativeNature,
    #[token = "idt_nature"]
    TimeIntegralNature,
    #[token = "units"]
    Units,
}

fn ignore_comments<'source, Src: Source<'source>>(lex: &mut logos::Lexer<Token, Src>) {
    //handel comment here since we dont want the resulting token anyway (if i don't do this lexer generation slows to a crawl)
    use logos::internal::LexerInternal;

    if lex.slice().as_bytes() == b"/*" {
        let mut lines = 0;
        loop {
            match lex.read() {
                None => return lex.token = Token::UnexpectedEOF,
                Some(b'*') => {
                    lex.bump(1);
                    if lex.read() == Some(b'/') {
                        lex.bump(1);
                        break;
                    }
                }
                Some(b'\n') => {
                    lines += 1;
                    lex.bump(1)
                }
                Some(_) => lex.bump(1),
            }
        }
        lex.extras.0 = lines; //one newline is implicit by the token
        lex.advance();
    } else if lex.slice().as_bytes() == b"//" {
        loop {
            match lex.read() {
                Some(b'\n') => {
                    lex.bump(1);
                    break;
                }
                _ => lex.bump(1),
            }
        }
        lex.token = Token::Newline
    }
}

pub struct Lexer<'lt> {
    internal: logos::Lexer<Token, &'lt str>,
}
impl<'lt> Lexer<'lt> {
    pub fn new(source: &'lt str) -> Self {
        Self {
            internal: Token::lexer(source),
        }
    }
    pub fn multi_line_count(&mut self) -> &mut LineNumber {
        &mut self.internal.extras.0
    }
    #[cfg(test)]
    pub fn new_test(source: &'lt str) -> Self {
        let mut res = Self::new(source);
        while res.token() == Token::Newline {
            res.advance()
        }
        res
    }
    pub fn advance(&mut self) {
        self.internal.advance();
    }
    pub fn peek(&self) -> (Range, Token) {
        let mut lexer = self.internal.clone();
        lexer.advance();
        let range = lexer.range();
        let range = Range {
            start: range.start as Index,
            end: range.end as Index,
        };
        (range, lexer.token)
    }
    #[cfg(test)]
    pub fn test_advance(&mut self) {
        self.advance();
        while self.token() == Token::Newline {
            self.advance()
        }
    }
    #[inline]
    pub fn token(&self) -> Token {
        self.internal.token
    }
    pub fn range(&self) -> Range {
        let internal_range = self.internal.range();
        Range {
            start: internal_range.start as Index,
            end: internal_range.end as Index,
        }
    }
    pub fn token_len(&self) -> Index {
        self.range().end - self.range().start
    }
    pub fn slice(&self) -> &str {
        unsafe { self.internal.source.get_unchecked(self.internal.range()) }
    } //This is save since we know the range of the current token is always within the current source
}

#[cfg(test)]
mod test {
    use super::*;

    #[test]
    pub fn macro_if() {
        let mut lexer = Lexer::new("`ifdef");
        assert_eq!(lexer.token(), Token::MacroIf)
    }
    #[test]
    pub fn macro_ifn() {
        let mut lexer = Lexer::new("`ifndef");
        assert_eq!(lexer.token(), Token::MacroIfn)
    }
    #[test]
    pub fn macro_else() {
        let mut lexer = Lexer::new("`else");
        assert_eq!(lexer.token(), Token::MacroElse)
    }
    #[test]
    pub fn macro_elsif() {
        let mut lexer = Lexer::new("`elsif");
        assert_eq!(lexer.token(), Token::MacroElsif)
    }
    #[test]
    pub fn macro_definition() {
        let mut lexer = Lexer::new("`define");
        assert_eq!(lexer.token(), Token::MacroDef)
    }
    #[test]
    pub fn include() {
        assert_eq!(Lexer::new("`include").token(), Token::Include)
    }
    #[test]
    pub fn simple_ident() {
        let mut lexer = Lexer::new_test("test _test  test2  test$\ntest2_$ iftest");
        assert_eq!(lexer.token(), Token::SimpleIdentifier);
        assert_eq!(lexer.slice(), "test");
        lexer.test_advance();
        assert_eq!(lexer.token(), Token::SimpleIdentifier);
        assert_eq!(lexer.slice(), "_test");
        lexer.test_advance();
        assert_eq!(lexer.token(), Token::SimpleIdentifier);
        assert_eq!(lexer.slice(), "test2");
        lexer.test_advance();
        assert_eq!(lexer.token(), Token::SimpleIdentifier);
        assert_eq!(lexer.slice(), "test$");
        lexer.test_advance();
        assert_eq!(lexer.token(), Token::SimpleIdentifier);
        assert_eq!(lexer.slice(), "test2_$");
        lexer.test_advance();
        assert_eq!(lexer.token(), Token::SimpleIdentifier);
        assert_eq!(lexer.slice(), "iftest");
    }
    #[test]
    pub fn escaped_ident() {
        let mut lexer = Lexer::new("\\lel\\\\lel \\if ");
        assert_eq!(lexer.token(), Token::EscapedIdentifier);
        assert_eq!(&lexer.slice()[1..9], "lel\\\\lel");
        lexer.advance();
        assert_eq!(lexer.token(), Token::EscapedIdentifier);
        assert_eq!(&lexer.slice()[1..3], "if");
    }
    #[test]
    pub fn comment() {
        let mut lexer = Lexer::new_test("//jdfjdfjw4$%\r%&/**#.,|\n/*3\n\r\n_/*_*/ test");
        assert_eq!(lexer.token(), Token::SimpleIdentifier);
        assert_eq!(lexer.slice(), "test")
    }
    #[test]
    pub fn string() {
        let mut lexer = Lexer::new(r#""lel\"dsd%§.,-032391\t    ""#);
        assert_eq!(lexer.token(), Token::LiteralString);
    }
    #[test]
    pub fn unsigned_number() {
        let mut lexer = Lexer::new("1_2345_5678_9");
        assert_eq!(lexer.token(), Token::LiteralUnsignedNumber);
    }
    #[test]
    pub fn macro_ref() {
        let mut lexer = Lexer::new_test("`lel");
        assert_eq!(lexer.token(), Token::MacroReference)
    }
    #[test]
    pub fn real_number() {
        let mut lexer = Lexer::new_test(
            "1.2
        0.1
2394.26331
1.2E12 // the exponent symbol can be e or E
1.30e-2
0.1e-0
236.123_763_e-12 // underscores are ignored
1.3u
23E10
29E-2
7k",
        );
        while lexer.token() != Token::EOF {
            assert_eq!(lexer.token(), Token::LiteralRealNumber);
            lexer.test_advance()
        }
    }
}
