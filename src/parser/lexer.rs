/*
 * ******************************************************************************************
 * Copyright (c) 2019 Pascal Kuthe. This file is part of the VARF project.
 * It is subject to the license terms in the LICENSE file found in the top-level directory
 *  of this distribution and at  https://gitlab.com/DSPOM/VARF/blob/master/LICENSE.
 *  No part of VARF, including this file, may be copied, modified, propagated, or
 *  distributed except according to the terms contained in the LICENSE file.
 * *****************************************************************************************
 */

use logos::internal::LexerInternal;
use logos::{Extras, Logos, Slice, Source};

use crate::span::{Index, LineNumber, Range};

#[derive(Clone, Copy)]
pub struct VerilogExtras {
    define_flag: bool,
    line_number: LineNumber,
}
impl Extras for VerilogExtras {}
impl Default for VerilogExtras {
    fn default() -> Self {
        Self {
            define_flag: false,
            line_number: 0,
        }
    }
}

//in terms of api this just serves as a lexer token enum. however it actually is the real lexer generated by logos.
#[derive(Clone, Logos, Debug, PartialEq, Copy)]
#[extras = "VerilogExtras"]
pub enum Token {
    //Newline handling
    #[regex = r"\\\n"]
    MacroDefNewLine,
    #[regex = r"\n"]
    Newline,
    CommentNewline,
    CommentEnd,

    //Actual Tokens

    //required rules
    #[end]
    EOF,
    #[token = "//"]
    #[token = "/*"]
    #[callback = "ignore_comments"]
    #[error]
    Unexpected,
    UnexpectedEOF,
    PreprocessorError,

    #[regex = r"`[a-zA-Z_][[:word:]\$]*"]
    MacroReference,
    //Compiler directives
    #[token = "`include"]
    Include,
    #[token = "`ifdef"]
    MacroIf,
    #[token = "`ifndef"]
    MacroIfn,
    #[token = "`elsif"]
    MacroElsif,
    #[token = "`else"]
    MacroElse,
    #[token = "`endif"]
    MacroEndIf,
    #[token = "`define"]
    #[callback = "handle_macro_def"]
    MacroDef,

    //Identifiers
    #[regex = r"[a-zA-Z_][[:word:]\$]*"]
    #[callback = "handle_simple_ident"]
    SimpleIdentifier,
    SimpleIdentWithBracket, //used for macro defintions where spaces matter (eg `define test(a,b) is not the same as `define test (a,b) as first expands to an empty stringa nd the second to (a,b)
    #[regex = r"\\[[:print:]&&\S]+\s"]
    EscapedIdentifier,
    #[regex = r"\$[a-zA-Z0-9_\$][a-zA-Z0-9_\$]*"]
    SystemCall,

    //Constants
    #[regex = r#""([^\n"\\]|\\[\\tn"])*""#]
    LiteralString,

    #[regex = r"[0-9][0-9_]*"]
    LiteralUnsignedNumber,
    #[regex = r"[0-9][0-9_]*\.[0-9][0-9_]*[TGMKkmupfa]"]
    LiteralRealNumberDotScaleChar,
    #[regex = r"[0-9][0-9_]*\.[0-9][0-9_]*[eE][+-]?[0-9][0-9_]*"]
    LiteralRealNumberDotExp,
    #[regex = r"[0-9][0-9_]*[TGMKkmupfa]"]
    LiteralRealNumberScaleChar,
    #[regex = r"[0-9][0-9_]*[eE][+-]?[0-9][0-9_]*"]
    LiteralRealNumberExp,
    #[regex = r"[0-9][0-9_]*\.[0-9][0-9_]*"]
    LiteralRealNumberDot,

    //Symbols
    #[token = "."]
    Accessor,
    #[token = ";"]
    Semicolon,
    #[token = ":"]
    Colon,
    #[token = ","]
    Comma,
    #[token = "("]
    ParenOpen,
    #[token = ")"]
    ParenClose,
    #[token = "(*"]
    AttributeStart,
    #[token = "*)"]
    AttributeEnd,
    #[token = "["]
    SquareBracketOpen,
    #[token = "]"]
    SquareBracketClose,
    #[token = "<+"]
    Contribute,
    #[token = "="]
    Assign,
    #[token = "#"]
    Hash,

    //Arithmatic Operators
    #[token = "*"]
    OpMul,
    #[token = "/"]
    OpDiv,
    #[token = "%"]
    OpModulus,
    #[token = "+"]
    Plus,
    #[token = "-"]
    Minus,
    #[token = "**"]
    OpExp,
    //UnaryOperators
    #[token = "!"]
    OpLogicNot,
    #[token = "~"]
    OpBitNot,

    #[token = "<<"]
    OpArithmeticShiftLeft,
    #[token = ">>"]
    OpArithmeticShiftRight,

    //Relational
    #[token = "<"]
    OpLess,
    #[token = "<="]
    OpLessEqual,
    #[token = ">"]
    OpGreater,
    #[token = ">="]
    OpGreaterEqual,
    #[token = "=="]
    OpEqual,
    #[token = "!="]
    OpNotEqual,
    //Logic
    #[token = "&&"]
    OpLogicAnd,
    #[token = "||"]
    OpLogicalOr,

    //Bit
    #[token = "&"]
    OpBitAnd,
    #[token = "^"]
    OpBitXor,
    #[token = "~^"]
    #[token = "^~"]
    OpBitNXor,
    #[token = "|"]
    OpBitOr,

    //Other
    #[token = "?"]
    OpCondition,

    //Keywords
    #[token = "if"]
    If,
    #[token = "else"]
    Else,

    #[token = "while"]
    While,

    #[token = "begin"]
    Begin,
    #[token = "end"]
    End,

    #[token = "module"]
    Module,
    #[token = "endmodule"]
    EndModule,
    #[token = "discipline"]
    Discipline,
    #[token = "enddiscipline"]
    EndDiscipline,

    #[token = "nature"]
    Nature,
    #[token = "endnature"]
    EndNature,

    #[token = "branch"]
    Branch,
    #[token = "parameter"]
    Parameter,
    #[token = "localparam"]
    DefineParameter,
    #[token = "defparam"]
    LocalParameter,

    #[token = "analog"]
    Analog,
    #[token = "initial"]
    AnalogInitial,

    #[token = "input"]
    Input,
    #[token = "inout"]
    Inout,
    #[token = "output"]
    Output,

    #[token = "signed"]
    Signed,
    #[token = "vectored"]
    Vectored,
    #[token = "scalared"]
    Scalared,

    //Types
    #[token = "string"]
    String,
    #[token = "time"]
    Time,
    #[token = "realtime"]
    Realtime,
    #[token = "integer"]
    Integer,
    #[token = "real"]
    Real,
    #[token = "reg"]
    Reg,
    #[token = "wreal"]
    Wreal,
    #[token = "supply0"]
    Supply0,
    #[token = "supply1"]
    Supply1,
    #[token = "tri"]
    Tri,
    #[token = "triand"]
    TriAnd,
    #[token = "trior"]
    TriOr,
    #[token = "tri0"]
    Tri0,
    #[token = "tri1"]
    Tri1,
    #[token = "wire"]
    Wire,
    #[token = "uwire"]
    Uwire,
    #[token = "wand"]
    Wand,
    #[token = "wor"]
    Wor,
    #[token = "ground"]
    Ground,

    #[token = "potential"]
    Potential,
    #[token = "flow"]
    Flow,
    #[token = "domain"]
    Domain,
    #[token = "discrete"]
    Discrete,
    #[token = "continuous"]
    Continuous,

    #[token = "ddt"]
    TimeDerivative,
    #[token = "ddx"]
    PartialDerivative,
    #[token = "idt"]
    TimeIntegral,
    #[token = "idtmod"]
    TimeIntegralMod,
    #[token = "limexp"]
    LimExp,
    #[token = "white_noise"]
    WhiteNoise,
    #[token = "flicker_noise"]
    FlickerNoise,

    #[token = "pow"]
    Pow,
    #[token = "sqrt"]
    Sqrt,

    #[token = "hypot"]
    Hypot,
    #[token = "exp"]
    Exp,
    #[token = "ln"]
    Ln,
    #[token = "log"]
    Log,
    #[token = "min"]
    Min,
    #[token = "max"]
    Max,
    #[token = "abs"]
    Abs,
    #[token = "floor"]
    Floor,
    #[token = "ceil"]
    Ceil,

    #[token = "sin"]
    Sin,
    #[token = "cos"]
    Cos,
    #[token = "tan"]
    Tan,

    #[token = "asin"]
    ArcSin,
    #[token = "acos"]
    ArcCos,
    #[token = "atan"]
    ArcTan,
    #[token = "atan2"]
    ArcTan2,

    #[token = "sinh"]
    SinH,
    #[token = "cosh"]
    CosH,
    #[token = "tanh"]
    TanH,

    #[token = "asinh"]
    ArcSinH,
    #[token = "acosh"]
    ArcCosH,
    #[token = "atanh"]
    ArcTanH,

    #[token = "from"]
    From,
    #[token = "exclude"]
    Exclude,
    #[token = "inf"]
    Infinity,
    #[token = "-inf"]
    MinusInfinity,

    #[token = "abstol"]
    Abstol,
    #[token = "access"]
    Access,
    #[token = "ddt_nature"]
    TimeDerivativeNature,
    #[token = "idt_nature"]
    TimeIntegralNature,
    #[token = "units"]
    Units,
}

#[inline]
fn ignore_comments<'source, Src: Source<'source>>(lex: &mut logos::Lexer<Token, Src>) {
    //handel comment here since we dont want the resulting token anyway (if i don't do this lexer generation slows to a crawl)

    if lex.slice().as_bytes() == b"/*" {
        let mut lines = 0;
        loop {
            match lex.read() {
                None => return lex.token = Token::UnexpectedEOF,
                Some(b'*') => {
                    lex.bump(1);
                    if lex.read() == Some(b'/') {
                        lex.bump(1);
                        break;
                    }
                }
                Some(b'\n') => {
                    lines += 1;
                    lex.bump(1)
                }
                Some(_) => lex.bump(1),
            }
        }
        if lines > 0 {
            lex.extras.line_number += lines - 1; //one newline is implicit by the token
            lex.token = Token::CommentNewline
        } else {
            lex.advance()
        }
    } else if lex.slice().as_bytes() == b"//" {
        loop {
            match lex.read() {
                Some(b'\n') => {
                    lex.bump(1);
                    break;
                }
                None => break,
                _ => lex.bump(1),
            }
        }
        lex.token = Token::Newline
    }
}
#[inline]
fn handle_simple_ident<'source, Src: Source<'source>>(lex: &mut logos::Lexer<Token, Src>) {
    if std::mem::take(&mut lex.extras.define_flag) {
        loop {
            match lex.read() {
                Some(b'/') => lex.advance(),
                Some(b'*') => lex.advance(),
                _ => break,
            }
            if lex.token == Token::CommentNewline {
                lex.advance();
            } else {
                break;
            }
        }
        if lex.read() == Some(b'(') {
            lex.bump(1);
            lex.token = Token::SimpleIdentWithBracket;
            return;
        }
    }
    lex.token = Token::SimpleIdentifier;
}
#[inline]
fn handle_macro_def<'source, Src: Source<'source>>(lex: &mut logos::Lexer<Token, Src>) {
    lex.extras.define_flag = true;
    lex.token = Token::MacroDef;
}

pub struct Lexer<'lt> {
    internal: logos::Lexer<Token, &'lt str>,
}
impl<'lt> Lexer<'lt> {
    pub fn new(source: &'lt str) -> Self {
        Self {
            internal: Token::lexer(source),
        }
    }

    #[cfg(test)]
    pub fn new_test(source: &'lt str) -> Self {
        let mut res = Self {
            internal: Token::lexer(source),
        };
        while res.token() == Token::Newline || res.token() == Token::CommentNewline {
            res.advance()
        }
        res
    }

    pub fn advance(&mut self) {
        if self.internal.extras.line_number > 0 {
            debug_assert_eq!(self.internal.token, Token::CommentNewline);
            self.internal.extras.line_number -= 1;
        } else {
            self.internal.advance();
        }
    }

    pub fn peek(&self) -> (Range, Token) {
        let mut lexer = self.internal.clone();
        lexer.advance();
        let range = lexer.range();
        let range = Range {
            start: range.start as Index,
            end: range.end as Index,
        };
        (range, lexer.token)
    }
    #[cfg(test)]
    pub fn test_advance(&mut self) {
        self.advance();
        while self.token() == Token::Newline || self.token() == Token::CommentNewline {
            self.advance()
        }
    }
    #[inline]
    pub fn token(&self) -> Token {
        self.internal.token
    }
    pub fn range(&self) -> Range {
        let internal_range = self.internal.range();
        Range {
            start: internal_range.start as Index,
            end: internal_range.end as Index,
        }
    }
    pub fn token_len(&self) -> Index {
        self.range().end - self.range().start
    }
    pub fn slice(&self) -> &str {
        unsafe { self.internal.source.get_unchecked(self.internal.range()) }
    } //This is save since we know the range of the current token is always within the current source
}

#[cfg(test)]
mod test {
    use super::*;

    #[test]
    pub fn macro_if() {
        let lexer = Lexer::new("`ifdef");
        assert_eq!(lexer.token(), Token::MacroIf)
    }
    #[test]
    pub fn macro_ifn() {
        let lexer = Lexer::new("`ifndef");
        assert_eq!(lexer.token(), Token::MacroIfn)
    }
    #[test]
    pub fn macro_else() {
        let lexer = Lexer::new("`else");
        assert_eq!(lexer.token(), Token::MacroElse)
    }
    #[test]
    pub fn macro_elsif() {
        let lexer = Lexer::new("`elsif");
        assert_eq!(lexer.token(), Token::MacroElsif)
    }
    #[test]
    pub fn macro_definition() {
        let lexer = Lexer::new("`define");
        assert_eq!(lexer.token(), Token::MacroDef)
    }
    #[test]
    pub fn include() {
        assert_eq!(Lexer::new("`include").token(), Token::Include)
    }
    #[test]
    pub fn simple_ident() {
        let mut lexer = Lexer::new_test("test _test  egta  test$\ntest2_$ iftest");
        assert_eq!(lexer.token(), Token::SimpleIdentifier);
        assert_eq!(lexer.slice(), "test");
        lexer.test_advance();
        assert_eq!(lexer.token(), Token::SimpleIdentifier);
        assert_eq!(lexer.slice(), "_test");
        lexer.test_advance();
        assert_eq!(lexer.token(), Token::SimpleIdentifier);
        assert_eq!(lexer.slice(), "egta");
        lexer.test_advance();
        assert_eq!(lexer.token(), Token::SimpleIdentifier);
        assert_eq!(lexer.slice(), "test$");
        lexer.test_advance();
        assert_eq!(lexer.token(), Token::SimpleIdentifier);
        assert_eq!(lexer.slice(), "test2_$");
        lexer.test_advance();
        assert_eq!(lexer.token(), Token::SimpleIdentifier);
        assert_eq!(lexer.slice(), "iftest");
    }
    #[test]
    pub fn escaped_ident() {
        let mut lexer = Lexer::new("\\lel\\\\lel \\if ");
        assert_eq!(lexer.token(), Token::EscapedIdentifier);
        assert_eq!(&lexer.slice()[1..9], "lel\\\\lel");
        lexer.advance();
        assert_eq!(lexer.token(), Token::EscapedIdentifier);
        assert_eq!(&lexer.slice()[1..3], "if");
    }
    #[test]
    pub fn comment() {
        let lexer = Lexer::new_test("//jdfjdfjw4$%\r%&/**#.,|\ntest");
        assert_eq!(lexer.token(), Token::SimpleIdentifier);
        assert_eq!(lexer.slice(), "test")
    }
    #[test]
    pub fn block_comment() {
        let mut lexer = Lexer::new_test("/*A\nB\n*C*/`test");
        assert_eq!(lexer.token(), Token::MacroReference);
        assert_eq!(lexer.slice(), "`test")
    }
    #[test]
    pub fn string() {
        let lexer = Lexer::new(r#""lel\"dsd%§.,-032391\t    ""#);
        assert_eq!(lexer.token(), Token::LiteralString);
    }
    #[test]
    pub fn unsigned_number() {
        let lexer = Lexer::new("1_2345_5678_9");
        assert_eq!(lexer.token(), Token::LiteralUnsignedNumber);
    }
    #[test]
    pub fn macro_ref() {
        let test = "`egta";

        let lexer = Lexer::new_test(test);
        assert_eq!(lexer.token(), Token::MacroReference)
    }
    #[test]
    pub fn real_number() {
        let mut lexer = Lexer::new_test(
            "1.2
            0.1
            2394.26331
            1.2E12 // the exponent symbol can be e or E
            1.30e-2
            0.1e-0
            236.123_763_e-12 // underscores are ignored
            1.3u
            23E10
            29E-2
            7k",
        );
        assert_eq!(lexer.token(), Token::LiteralRealNumberDot);
        lexer.test_advance();
        assert_eq!(lexer.token(), Token::LiteralRealNumberDot);
        lexer.test_advance();
        assert_eq!(lexer.token(), Token::LiteralRealNumberDot);
        lexer.test_advance();
        assert_eq!(lexer.token(), Token::LiteralRealNumberDotExp);
        lexer.test_advance();
        assert_eq!(lexer.token(), Token::LiteralRealNumberDotExp);
        lexer.test_advance();
        assert_eq!(lexer.token(), Token::LiteralRealNumberDotExp);
        lexer.test_advance();
        assert_eq!(lexer.token(), Token::LiteralRealNumberDotExp);
        lexer.test_advance();
        assert_eq!(lexer.token(), Token::LiteralRealNumberDotScaleChar);
        lexer.test_advance();
        assert_eq!(lexer.token(), Token::LiteralRealNumberExp);
        lexer.test_advance();
        assert_eq!(lexer.token(), Token::LiteralRealNumberExp);
        lexer.test_advance();
        assert_eq!(lexer.token(), Token::LiteralRealNumberScaleChar);
    }
}
